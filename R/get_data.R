
#' Get stations data
#'
#' \code{get_stations} returns a dataframe with data from the stations that
#' exist in a database of Hydroscope.
#'
#' @param subdomain One of the subdomains of hydroscope.gr
#'
#' @return If \code{subdomain} is one of \code{"kyy"}, \code{"ypaat"} or
#' \code{"emy"},returns a tidy dataframe with stations' data from the
#' corresponding database of hydroscope.gr. Otherwise gives an error message.
#'
#' The dataframe columns are:
#' \describe{
#'     \item{ID}{The station's ID from the domain's database}
#'     \item{Name}{The station's name}
#'     \item{WaterDivisionID}{The station's Water Division ID, values: GR01 -
#'     GR14}
#'     \item{WaterBasin}{The station's Water Basin}
#'     \item{PoliticalDivision}{The station's Political Division}
#'     \item{Owner}{The station's owner}
#'     \item{Type}{The station's type}
#' }
#'
#' @note
#' Stations' IDs might not be unique at the different databases records from the
#' different Hydroscope domains.
#'
#' The Greek Water Divisions ID are:
#' \tabular{ll}{
#' \strong{Code}  \tab \strong{Name} \cr
#' GR01 \tab DYTIKE PELOPONNESOS \cr
#' GR02 \tab BOREIA PELOPONNESOS \cr
#' GR03 \tab ANATOLIKE PELOPONNESOS \cr
#' GR04 \tab DYTIKE STEREA ELLADA \cr
#' GR05 \tab EPEIROS  \cr
#' GR06 \tab ATTIKE  \cr
#' GR07 \tab ANATOLIKE STEREA ELLADA \cr
#' GR08 \tab THESSALIA  \cr
#' GR09 \tab DYTIKE MAKEDONIA  \cr
#' GR10 \tab KENTRIKE MAKEDONIA  \cr
#' GR11 \tab ANATOLIKE MAKEDONIA  \cr
#' GR12 \tab THRAKE  \cr
#' GR13 \tab KRETE  \cr
#' GR14 \tab NESOI AIGAIOU  \cr
#' }
#'
#' The codes used in Owner variable are:
#' \tabular{ll}{
#' \strong{Code}  \tab \strong{Name} \cr
#' min_env \tab Ministry of Environment and Energy \cr
#' noa \tab National Observatory of Athens \cr
#' min_rur \tab  Ministry of Rural Development \cr
#' prefec \tab Prefectures of Greece  \cr
#' emy \tab National Meteorological Service  \cr
#' dei \tab Public Power Corporation  \cr
#' }
#'
#' The types used at station's type are:
#' \tabular{ll}{
#' \strong{Code}  \tab \strong{Description} \cr
#' meteo_station \tab Weather station \cr
#' stream_gage \tab Gauging station  \cr
#' }
#'
#' @examples
#' # get stations' data from the Greek Ministry of Environment and Energy
#' kyy_stations <- get_stations()
#'
#' # get stations' data from the Greek National Meteorological Service
#' emy_stations <- get_stations(subdomain = "emy")
#'
#' \dontrun{
#' dei_stations <- get_stations(subdomain = "dei")
#' }
#'
#' @references
#' European Terrestrial Reference System 1989 (ETRS),
#' \url{http://bit.ly/2kJwFuf}
#'
#' Stations' data are retrieved from the Hydroscope's databases:
#' \itemize{
#' \item Ministry of Environment, Energy and Climate Change,
#' \url{http://kyy.hydroscope.gr}
#' \item Ministry of Rural Development and Food,
#' \url{http://ypaat.hydroscope.gr}
#' \item National Meteorological Service,
#' \url{http://emy.hydroscope.gr}
#'}
#' Greek Water Divisions,
#' \url{http://bit.ly/2kk0tOm}
#'
#'
#' @author Konstantinos Vantas, \email{kon.vantas@gmail.com}
#' @import XML
#' @export get_stations
get_stations <- function(subdomain =  c("kyy", "ypaat", "emy")) {

  # match subdomain values -----------------------------------------------------
  subdomain <- match.arg(subdomain)
  url <- hydroscope_url(subdomain)

  # Web scrapping --------------------------------------------------------------
  doc <- XML::htmlParse(url, encoding = "UTF8")

  # get table nodes
  html_table <- "/html/body/div[3]/div/div/div/div[2]/div/table"
  table_nodes <- XML::getNodeSet(doc, html_table)

  # check table nodes
  if (is.null(table_nodes)) {
    warning(paste0("Couldn't download stations' list from", url))
    return(stationsNA)
  }

  # read html file to table
  stations <- XML::readHTMLTable(table_nodes[[1]], stringsAsFactors = FALSE)

  # Make valid names -----------------------------------------------------------

  # make valid names for stations if there are rows and the columns are 7
  if (NROW(stations) > 0 & NCOL(stations) == 7) {
    names(stations) <- c("ID", "Name", "WaterBasin", "WaterDivision",
                         "PoliticalDivision", "Owner", "Type")
  } else {
    warning(paste("Couldn't get expected station's table from: ", url))
    return(stationsNA)
  }

  # Translations and transliterations ------------------------------------------

  # replace greek characters with latin
  for (cname in c(names(stations)[-1])) {
    stations[cname] <- greek2latin(stations[cname])
  }

  # translate types
  stations$Type <- stations_types(stations$Type)

  # add water division id
  stations$WaterDivisionID <- add_wd_id(stations$WaterDivision)

  # use abr/sions for owners' names
  stations$Owner <- owner_names(stations$Owner)

  # remove area from water basin values
  stations$WaterBasin <- sapply(stations$WaterBasin, function(str){
    gsub("\\([^()]*\\)", "", str)
    })

  # Return data ----------------------------------------------------------------
  cnames <- c("ID", "Name", "WaterDivisionID", "WaterBasin",
              "PoliticalDivision", "Owner", "Type")

  return(stations[cnames])
}


#' Get station's coordinates
#'
#' @param subdomain
#' @param stationID
#'
#' @return
#' #' The dataframe columns are:
#' \describe{
#'     \item{ID}{The stationID from the database}
#'     \item{Long}{The station's longitude in decimal degrees, ETRS89}
#'     \item{Lat}{The station's latitude in decimal degrees, ETRS89}
#'     \item{Elevation}{The station's altitude, meters above sea level}
#' }
#' @export
#'
#' @examples
get_coords <- function(subdomain =  c("main", "kyy", "ypaat", "emy"),
                       stationID) {

  # check that stationID is given
  if(is.null(stationID)) stop("argument \"stationID\" is missing")

  # match subdomain values -----------------------------------------------------
  subdomain <- match.arg(subdomain)
  url <- hydroscope_url(subdomain)
  url <- paste0(url,"/stations/d/", stationID, "/")

  # Web scrapping --------------------------------------------------------------
  url <- paste0("http://kyy.hydroscope.gr/stations/d/", stationID, "/")
  doc <- XML::htmlParse(url, encoding = "UTF8")

  # get table nodes
  path <- "/html/body/div[3]/div/div[1]/div/div[2]/div/div[2]/table"
  tableNodes <- XML::getNodeSet(doc, path)

  # check table nodes
  if (is.null(tableNodes)) {
    warning(paste0("Couldn't download station data for stationID =", stationID))
    res <- data.frame(ID = stationID, Long = NA, Lat = NA, Elevation = NA)
    return(res)
  }

  # read table
  stat_table <- XML::readHTMLTable(tableNodes[[1]], header = FALSE,
                                   stringsAsFactors = FALSE)

  # get values from table
  values <- stat_table$V2
  names(values) <- stat_table$V1

  # Coordinates creation -------------------------------------------------------

  # convert Coords to Lat and Long
  Elevation <- as.numeric(values["Altitude"])
  Coords <- as.character(values["Co-ordinates"])
  tmp <- stringr::str_split(string = Coords, pattern = ",|\n", simplify = TRUE)
  Lat <- as.numeric(tmp[1])
  Long <- as.numeric(tmp[2])

  # return results as a dataframe
  data.frame(ID = stationID, Long = Long, Lat = Lat, Elevation = Elevation)

}


#' Get timeseries corresponding to a station
#'
#' @param subdomain
#' @param stationID
#'
#' @return
#' @export
#'
#' @examples
get_timeseries <- function(subdomain =  c("main", "kyy", "ypaat", "emy"),
                           stationID) {

  # check that stationID is given
  if(is.null(stationID)) stop("argument \"stationID\" is missing")

  # match subdomain values -----------------------------------------------------
  subdomain <- match.arg(subdomain)
  url <- hydroscope_url(subdomain)
  url <- paste0(url,"/stations/d/", stationID, "/")

  # Web scrapping --------------------------------------------------------------
  url <- paste0("http://kyy.hydroscope.gr/stations/d/", stationID, "/")
  doc <- XML::htmlParse(url, encoding = "UTF8")

  # get table nodes
  tableNodes <- XML::getNodeSet(doc, "//*[@id=\"timeseries\"]")
  if (is.null(tableNodes)) {
    warning(paste("Couldn't download timeseries list for station ID =",
                  stationID))
    return(timeserNA(stationID))
  }
  # read table
  tb2 <- XML::readHTMLTable(tableNodes[[1]], header = TRUE,
                            stringsAsFactors = FALSE)
  tb2$StationID <- stationID

  # make valid names for timeseries --------------------------------------------
  if (NROW(tb2) > 0 & NCOL(tb2) == 10) {
    names(tb2) <- c("TimeSeriesID", "Name", "Variable", "TimeStep", "Unit",
                    "Remarks", "Instrument", "StartDate", "EndDate",
                    "StationID")
  } else {
    warning(paste("Couldn't get expected timeseries table from url: ", url, ""))
    return(timeserNA(stationID))
  }

  # Translations and transliterations ------------------------------------------
  for (cname in c(names(tb2))) {
    tb2[cname] <- greek2latin(tb2[cname])
  }

  # remove columns without data
  tb2$Name <- NULL
  tb2$Remarks <- NULL

  # translations
  tb2$Variable <- ts_variable(tb2$Variable)
  tb2$TimeStep <- ts_timestep(tb2$TimeStep)

  # convert start and end dates to posixct -------------------------------------
  tb2$StartDate <- ifelse(tb2$StartDate == "", NA, tb2$StartDate)
  tb2$EndDate <- ifelse(tb2$EndDate == "", NA, tb2$EndDate)

  time_format <- "%Y/%m/%d %H:%M"
  tb2$StartDate <- as.POSIXct(tb2$StartDate, format = time_format, tz = "")
  tb2$EndDate <- as.POSIXct(tb2$EndDate, format = time_format, tz = "")

  return(tb2)
}


#' Get timeseries data in a tidy dataframe
#'
#' @param subdomain
#' @param timeID
#'
#' @return
#' @export
#'
#' @examples
get_data <- function(subdomain =  c("kyy", "ypaat", "emy"), timeID) {

  # check that stationID is given
  if(is.null(timeID)) stop("argument \"timeID\" is missing")

  # match subdomain values -----------------------------------------------------
  subdomain <- match.arg(subdomain)
  url <- hydroscope_url(subdomain)
  url <- paste0(url, "/timeseries/d/", timeID, "/download/")

  # Download and convert hydroscope file to dataframe --------------------------

  # create a temp file
  tmp <- tempfile()
  suppressWarnings(
    result <- tryCatch({

      # download file
      dl_code <- utils::download.file(url = url, destfile = tmp)
      if(dl_code != 0) stop()

      # Count the number of fields in each line of a file
      cf <- readr::count_fields(tmp,
                                tokenizer = readr::tokenizer_csv(),
                                n_max = 50)

      # check the number of columns in hts file
      if(any(cf == 3)) {

        # read timeseries data
        tmFormat <- "%Y-%m-%d %H:%M"
        result <- readr::read_csv(file = tmp,
                                  skip =  sum(cf < 3),
                                  col_names = c('Date', 'Value', 'Comment'),
                                  col_types = list(
                                    readr::col_datetime(format = tmFormat),
                                    readr::col_double(),
                                    readr::col_character()))

        # remove NA Date values
        result <- result[!is.na(result$Date), ]

      } else {
        result <- dataNA()
      }

    }, error = function(e) {
      # return NA values
      warning(paste("Couldn't get timeseries' data from "), url)
      result <- dataNA()

    }, finally = {
      # delete temp file
      unlink(tmp)
    })
  )
  return(result)
}
